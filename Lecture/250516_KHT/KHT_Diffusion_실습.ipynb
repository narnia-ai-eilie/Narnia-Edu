{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyONa+Q0Dba+rfPIBZ+B7wqn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/narnia-ai-eilie/Narnia-Edu/blob/main/Lecture/250516_KHT/KHT_Diffusion_%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실전 생성 AI : 2D Diffusion\n",
        "\n",
        "* 날짜:\n",
        "* 이름:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1PpOHlxlnDvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(0) Environment Setup**\n",
        "---"
      ],
      "metadata": {
        "id": "XxQZXje__Db7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| 라이브러리 설치**"
      ],
      "metadata": {
        "id": "qN_MmRu1n5C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade diffusers[torch]"
      ],
      "metadata": {
        "id": "lQw5SIU3JnLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "vp_dCjGB9NrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| Utils**"
      ],
      "metadata": {
        "id": "Gxssow6GoL7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import random\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    # Python의 내장 난수 생성기 시드 설정\n",
        "    random.seed(seed)\n",
        "\n",
        "    # NumPy 난수 생성기 시드 설정\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # PyTorch 난수 생성기 시드 설정 (CPU)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # PyTorch 난수 생성기 시드 설정 (GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # 모든 GPU에 대해 시드 설정\n",
        "\n",
        "    # CuDNN 설정\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def draw_img_from_cond(skin, geo, cmap='gray'):\n",
        "    plt.figure(figsize=(3,3))\n",
        "    path = f'carhood/skin_{skin}_geometry_{geo}.npy'\n",
        "    img = np.load(path)\n",
        "    plt.imshow(img, cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "    return img\n",
        "\n",
        "\n",
        "def draw_dist(inputs, samples=100):\n",
        "    if len(inputs) > samples:\n",
        "        inputs = random.sample(list(inputs), samples)\n",
        "\n",
        "    pixel_values = []\n",
        "    for item in inputs:\n",
        "        if isinstance(item, Image.Image):\n",
        "            pixel_values.extend(list(item.getdata()))\n",
        "        elif isinstance(item, np.ndarray):\n",
        "            pixel_values.extend(item.flatten())\n",
        "        elif torch.is_tensor(item):\n",
        "            if len(item.shape) == 4 and item.shape[1] == 1:  # 이미지가 (batch_size, 1, height, width) 꼴일 때\n",
        "                item = item.squeeze(1)\n",
        "            elif len(item.shape) == 3:  # 이미지가 (batch_size, height, width) 꼴일 때\n",
        "                item = item.unsqueeze(1)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported image shape. Supported shapes: (batch_size, height, width) or (batch_size, 1, height, width).\")\n",
        "            item = item.detach().cpu().numpy()\n",
        "            pixel_values.extend(item.flatten())\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported image type. Supported types: PIL Image, numpy array, torch tensor.\")\n",
        "\n",
        "    plt.figure(figsize=(5,3))\n",
        "    plt.hist(pixel_values, bins=20, color='blue', alpha=0.7)\n",
        "    plt.xlabel('Pixel Value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Image Pixel Value Distribution')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def show_img(images_list,\n",
        "             r=1,\n",
        "             cmap='gray',\n",
        "             img_size=(5, 5),\n",
        "             axis=\"off\",\n",
        "             colorbar=False,\n",
        "             colorbar_range=None,\n",
        "             save_path=None):\n",
        "    if r < 1:\n",
        "        r = 1\n",
        "\n",
        "    total_images = len(images_list)\n",
        "    if total_images == 0:\n",
        "        print(\"No images to display.\")\n",
        "        return\n",
        "\n",
        "    cols = (total_images + r - 1) // r\n",
        "    fig, axs = plt.subplots(r, cols, figsize=(cols * img_size[0], r * img_size[1]))\n",
        "\n",
        "    if r == 1:\n",
        "        axs = axs.reshape(1, -1)\n",
        "\n",
        "    for idx, item in enumerate(images_list):\n",
        "        ax = axs[0, idx] if r == 1 else axs[idx // cols, idx % cols]\n",
        "        im = None\n",
        "        if isinstance(item, Image.Image):\n",
        "            if item.mode in ['L', '1']:  # Grayscale images\n",
        "                im = ax.imshow(item, cmap=cmap)\n",
        "            else:  # Color images\n",
        "                im = ax.imshow(item)\n",
        "        elif isinstance(item, np.ndarray):\n",
        "            if item.ndim == 2:  # 2D array, grayscale image\n",
        "                im = ax.imshow(item, cmap=cmap, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 3:  # 3D array, color image\n",
        "                im = ax.imshow(item, cmap=cmap if item.shape[-1] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 4 and item.shape[0] == 1:  # 4D array with batch dimension of 1\n",
        "                im = ax.imshow(item[0], cmap=cmap if item.shape[1] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported numpy array shape: {item.shape}.\")\n",
        "        elif torch.is_tensor(item):\n",
        "            item = item.detach().cpu().numpy()\n",
        "            if item.ndim == 2:  # 2D tensor, grayscale image\n",
        "                im = ax.imshow(item, cmap=cmap, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 3:  # 3D tensor, color image\n",
        "                im = ax.imshow(item.transpose(1, 2, 0), cmap=cmap if item.shape[0] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 4 and item.shape[0] == 1:  # 4D tensor with batch dimension of 1\n",
        "                im = ax.imshow(item[0].transpose(1, 2, 0), cmap=cmap if item.shape[1] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported torch tensor shape: {item.shape}.\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported image type. Supported types: PIL Image, numpy array, torch tensor.\")\n",
        "\n",
        "        if colorbar and im is not None:\n",
        "            divider = make_axes_locatable(ax)\n",
        "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "            fig.colorbar(im, cax=cax)\n",
        "\n",
        "        ax.axis(axis)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # 이미지 저장\n",
        "    if save_path is not None:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def load_img(path):\n",
        "    \"\"\"지정된 경로의 이미지를 로드하여 반환합니다.\"\"\"\n",
        "    try:\n",
        "        img = plt.imread(path)\n",
        "        img = img / 255.\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"이미지를 불러오는 중 오류가 발생했습니다: {e}\")\n",
        "        return None\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "def show_img(images_list,\n",
        "             r=1,\n",
        "             cmap='gray',\n",
        "             img_size=(5, 5),\n",
        "             axis=\"off\",\n",
        "             colorbar=False,\n",
        "             colorbar_range=None,\n",
        "             save_path=None,\n",
        "             titles=[]):  # titles 추가\n",
        "    if r < 1:\n",
        "        r = 1\n",
        "\n",
        "    total_images = len(images_list)\n",
        "    if total_images == 0:\n",
        "        print(\"No images to display.\")\n",
        "        return\n",
        "\n",
        "    cols = (total_images + r - 1) // r\n",
        "    fig, axs = plt.subplots(r, cols, figsize=(cols * img_size[0], r * img_size[1]))\n",
        "\n",
        "    # Adjust the axs shape for single subplot cases\n",
        "    if r == 1 and cols == 1:\n",
        "        axs = np.array([axs])\n",
        "\n",
        "    for idx, item in enumerate(images_list):\n",
        "        ax = axs.flatten()[idx]\n",
        "        im = None\n",
        "        if isinstance(item, Image.Image):\n",
        "            if item.mode in ['L', '1']:  # Grayscale images\n",
        "                im = ax.imshow(item, cmap=cmap)\n",
        "            else:  # Color images\n",
        "                im = ax.imshow(item)\n",
        "        elif isinstance(item, np.ndarray):\n",
        "            if item.ndim == 2:  # 2D array, grayscale image\n",
        "                im = ax.imshow(item, cmap=cmap, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 3:  # 3D array, color image\n",
        "                if item.shape[-1] == 1:  # Grayscale image with single channel\n",
        "                    im = ax.imshow(item.squeeze(), cmap=cmap, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "                else:  # Color image\n",
        "                    im = ax.imshow(item, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 4 and item.shape[0] == 1:  # 4D array with batch dimension of 1\n",
        "                im = ax.imshow(item[0].transpose(1, 2, 0), cmap=cmap if item.shape[1] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported numpy array shape: {item.shape}.\")\n",
        "        elif torch.is_tensor(item):\n",
        "            item = item.detach().cpu().numpy()\n",
        "            if item.ndim == 2:  # 2D tensor, grayscale image\n",
        "                im = ax.imshow(item, cmap=cmap, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 3:  # 3D tensor, color image\n",
        "                im = ax.imshow(item.transpose(1, 2, 0), cmap=cmap if item.shape[0] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 4 and item.shape[0] == 1:  # 4D tensor with batch dimension of 1\n",
        "                im = ax.imshow(item[0].transpose(1, 2, 0), cmap=cmap if item.shape[1] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported torch tensor shape: {item.shape}.\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported image type. Supported types: PIL Image, numpy array, torch tensor.\")\n",
        "\n",
        "        # Add title if titles list is provided\n",
        "        if titles and idx < len(titles):\n",
        "            ax.set_title(titles[idx], fontsize=10)\n",
        "\n",
        "        if colorbar and im is not None:\n",
        "            divider = make_axes_locatable(ax)\n",
        "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "            fig.colorbar(im, cax=cax)\n",
        "\n",
        "        ax.axis(axis)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # 이미지 저장\n",
        "    if save_path is not None:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "2O9iCBqiRxPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(0)"
      ],
      "metadata": {
        "id": "j7GAt_wAof-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| Dataset download**"
      ],
      "metadata": {
        "id": "3xPeogqyoTr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder https://drive.google.com/drive/u/0/folders/1vd1SmXp4L_xJXwDjoCzSoDfYziFDo5sS"
      ],
      "metadata": {
        "id": "GRS7fwBBPOJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip KHT/PTN.zip -d ."
      ],
      "metadata": {
        "id": "l6qJwSzDopo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(1) Dataset**"
      ],
      "metadata": {
        "id": "vvsJn35E_F_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| EDA**\n",
        "\n"
      ],
      "metadata": {
        "id": "J4BD_y6coTZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paths = glob.glob('./PTN/*jpg')\n",
        "print(len(paths))\n",
        "\n",
        "img = load_img(paths[6])\n",
        "print(img.shape)\n",
        "show_img([img], axis=True, colorbar=True)"
      ],
      "metadata": {
        "id": "8m48zdiHR5vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Groove 추출"
      ],
      "metadata": {
        "id": "fYJU6qFpNF2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_groove(im, void=2, thred=10):\n",
        "    bk = np.zeros_like(im)\n",
        "    for i, r in enumerate(im):\n",
        "        if np.sum(r) <=thred:\n",
        "            bk[i-void:i+void,:]=1\n",
        "    return bk"
      ],
      "metadata": {
        "id": "IcF4iP4Wol__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groove_img = get_groove(img)\n",
        "line_img = np.clip(groove_img + img, 0, 1)\n",
        "show_img([img, groove_img, line_img], colorbar=True)"
      ],
      "metadata": {
        "id": "SzENlqxWomF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 선 추출"
      ],
      "metadata": {
        "id": "6SKwwgXFNJsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def adjust_thickness(image_array, kernel_size=5):\n",
        "    \"\"\"\n",
        "    넘파이 배열 (0~1) 이미지를 입력받아 굵기 조절 (확장/축소) 적용.\n",
        "\n",
        "    Parameters:\n",
        "        - image_array (numpy.ndarray): 0~1 범위의 (n, n) 흑백 이미지\n",
        "        - kernel_size (int): 필터 크기 (클수록 강하게 적용)\n",
        "        - operation (str): 'dilate' (굵게) or 'erode' (얇게)\n",
        "\n",
        "    Returns:\n",
        "        - modified_image (numpy.ndarray): 변환된 0~1 범위 이미지\n",
        "    \"\"\"\n",
        "    # 0~1 → 0~255로 변환 (OpenCV는 0~255 범위 사용)\n",
        "    img = (image_array * 255).astype(np.uint8)\n",
        "\n",
        "    # 커널 생성 (커널 크기 조절 가능)\n",
        "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "\n",
        "    # 굵기 조절 연산 적용\n",
        "    modified_img = cv2.dilate(img, kernel, iterations=1)\n",
        "\n",
        "    # 결과를 다시 0~1 범위로 변환\n",
        "    return modified_img.astype(np.float32) / 255.0"
      ],
      "metadata": {
        "id": "2IaQvckjomII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_img = adjust_thickness(line_img, kernel_size=5)\n",
        "condition_img = np.clip(simple_img - groove_img, 0, 1)\n",
        "show_img([img, simple_img, condition_img], colorbar=True)"
      ],
      "metadata": {
        "id": "u3_Un8SczCKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| DataLoader**"
      ],
      "metadata": {
        "id": "N4wEiGoxJGw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import glob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "\n",
        "\n",
        "def get_groove(im, void=2, thred=10):\n",
        "    \"\"\"\n",
        "    이미지 내 밝기 값이 특정 임계값 이하인 행(row)을 중심으로\n",
        "    '홈' 영역을 마스크로 반환합니다.\n",
        "\n",
        "    Parameters:\n",
        "        im (ndarray): 입력 이미지 (2D grayscale 또는 RGB 단일 채널)\n",
        "        void (int): 탐지된 행을 중심으로 마스크를 확장할 범위 (상하 각 void픽셀)\n",
        "        thred (int): 한 행의 밝기 합계가 이 값 이하이면 홈으로 판단\n",
        "\n",
        "    Returns:\n",
        "        bk (ndarray): 입력 이미지와 동일한 크기의 마스크 배열 (홈 영역은 1, 나머지는 0)\n",
        "    \"\"\"\n",
        "    # 출력 마스크 초기화 (입력 이미지와 동일한 shape, 값은 모두 0)\n",
        "    bk = np.zeros_like(im)\n",
        "\n",
        "    # 각 행(r)을 순회하며 밝기 합 검사\n",
        "    for i, r in enumerate(im):\n",
        "        if np.sum(r) <= thred:\n",
        "            # 조건을 만족하는 경우 해당 행의 위아래 void 픽셀 범위 마스크를 1로 설정\n",
        "            bk[i - void:i + void, :] = 1\n",
        "\n",
        "    return bk\n",
        "\n",
        "def adjust_thickness(image_array, kernel_size=5):\n",
        "    \"\"\"\n",
        "    넘파이 배열 (0~1) 이미지를 입력받아 굵기 조절 (확장/축소) 적용.\n",
        "\n",
        "    Parameters:\n",
        "        - image_array (numpy.ndarray): 0~1 범위의 (n, n) 흑백 이미지\n",
        "        - kernel_size (int): 필터 크기 (클수록 강하게 적용)\n",
        "        - operation (str): 'dilate' (굵게) or 'erode' (얇게)\n",
        "\n",
        "    Returns:\n",
        "        - modified_image (numpy.ndarray): 변환된 0~1 범위 이미지\n",
        "    \"\"\"\n",
        "    # 0~1 → 0~255로 변환 (OpenCV는 0~255 범위 사용)\n",
        "    img = (image_array * 255).astype(np.uint8)\n",
        "\n",
        "    # 커널 생성 (커널 크기 조절 가능)\n",
        "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "\n",
        "    # 굵기 조절 연산 적용\n",
        "    modified_img = cv2.dilate(img, kernel, iterations=1)\n",
        "\n",
        "    # 결과를 다시 0~1 범위로 변환\n",
        "    return modified_img.astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class PP(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 paths=[],\n",
        "                 return_condition=True,\n",
        "                 resize=None,\n",
        "                 batch_size: int = 4,\n",
        "                 shuffle: bool = True,\n",
        "                 aug=True,\n",
        "                 dtype = torch.float16,\n",
        "                ):\n",
        "\n",
        "        # init\n",
        "        self.paths = paths\n",
        "\n",
        "        self.return_condition = return_condition\n",
        "        self.aug = aug\n",
        "        self.resize = resize\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.dtype = dtype\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        path = self.paths[idx]\n",
        "        img = load_img(path)\n",
        "        groove_img = get_groove(img)\n",
        "        line_img = np.clip(groove_img + img, 0, 1)\n",
        "        simple_img = adjust_thickness(line_img, kernel_size=5)\n",
        "        condition_img = np.clip(simple_img - groove_img, 0, 1)\n",
        "\n",
        "        img = torch.tensor(img, dtype=self.dtype).unsqueeze(0)           # (1, H, W)\n",
        "        img = ( img - 0.5 ) * 2.\n",
        "        condition_img = torch.tensor(condition_img, dtype=self.dtype).unsqueeze(0)\n",
        "\n",
        "        if self.aug:\n",
        "            _, h, w = img.shape\n",
        "            max_x = w - h\n",
        "            start_x = torch.randint(0, max_x + 1, (1,)).item()\n",
        "\n",
        "            img = img[:, :, start_x:start_x + 512]\n",
        "            condition_img = condition_img[:, :, start_x:start_x + 512]\n",
        "\n",
        "            if torch.rand(1) < 0.5:\n",
        "                img = torch.flip(img, dims=[2])              # width 방향\n",
        "                condition_img = torch.flip(condition_img, dims=[2])\n",
        "\n",
        "            if torch.rand(1) < 0.5:\n",
        "                img = torch.flip(img, dims=[1])              # height 방향\n",
        "                condition_img = torch.flip(condition_img, dims=[1])\n",
        "\n",
        "\n",
        "        if self.resize is not None:\n",
        "            img = transforms.Resize(self.resize)(img)\n",
        "            condition_img = transforms.Resize(self.resize)(condition_img)\n",
        "\n",
        "        if self.return_condition:\n",
        "            return img, condition_img\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "\n",
        "    def get_loader(self):\n",
        "        return DataLoader(self, batch_size=self.batch_size, shuffle=self.shuffle)\n",
        "\n",
        "    def get_batch(self, idx=0):\n",
        "        ds = self.get_loader()\n",
        "        for i, batch in enumerate(ds):\n",
        "            if i == idx:\n",
        "                break\n",
        "        return batch"
      ],
      "metadata": {
        "id": "kBXB9HPJSizy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "paths= glob.glob('./PTN/*jpg')\n",
        "pp = PP(paths, resize=256, shuffle=False)\n",
        "for i in range(3):\n",
        "    print('-- trial', i)\n",
        "    img, condition = pp.get_batch(0)\n",
        "    print(img.shape, condition.shape)\n",
        "    show_img(img, colorbar=True, axis=True)\n",
        "    show_img(condition, colorbar=True, axis=True)"
      ],
      "metadata": {
        "id": "nbz9a30YHdLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(2) Modules**\n",
        "\n",
        "![](https://github.com/EilieYoun/Narnia-Edu/blob/main/imgs/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202024-08-23%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%202.46.03.png?raw=true)\n",
        "\n",
        "\n",
        "![](https://github.com/EilieYoun/box/blob/main/images/240214_ddpm.png?raw=true)\n",
        "\n",
        "\n",
        "**Diffusion**은 이미지를 점진적으로 정제하는 과정을 통해 노이즈를 제거하고, 원래 이미지를 복원하는 생성 모델입니다. 이번 시간에서는 **DDIM** (Denoising Diffusion Implicit Models)을 사용하여 노이즈를 추가하고 제거하는 과정을 수행합니다.\n",
        "\n",
        "이번 실습에서는 **Hugging Face**에서 제공하는 `diffusers` 라이브러리를 사용하여 **Diffusion** 모델을 구축합니다. 이 라이브러리는 다양한 **Diffusion** 모델을 쉽게 구현하고 실험할 수 있는 도구를 제공합니다. `diffusers` 라이브러리는 모듈화된 구조를 가지고 있어, 다양한 모델 구성 요소를 쉽게 설정하고 사용할 수 있습니다. 자세한 내용은 https://github.com/huggingface 에서 확인할 수 있습니다.\n"
      ],
      "metadata": {
        "id": "iWGe2_DQJJXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| UNet**\n",
        "\n",
        "첫 번째 모듈은 UNet입니다. UNet은 이미지 분할 및 재구성 작업에 널리 사용되는 네트워크 구조로, 이번 실습에서는 노이즈를 예측하는 데 사용됩니다. 모듈에 관한 자세한 내용은 [Hugging Face UNET 페이지](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_2d.py)에서 확인할 수 있습니다.\n",
        "\n",
        "- `UNet2DModel`을 사용하여 **UNet** 모델을 정의합니다.\n",
        "- 입력이미지와 채널 수를 적절하게 정의해 줍니다.\n",
        "- `layers_per_block`과 `block_out_channels`를 기존보다 작게 설정해 가벼운 모델을 만듭니다."
      ],
      "metadata": {
        "id": "OOYECSxfJR4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "unet = UNet2DModel(\n",
        "    sample_size = 128, # 이미지 데이터의 사이즈\n",
        "    in_channels = 1, # 이미지 채널 수\n",
        "    out_channels = 1, # 이미지 채널 수\n",
        "    layers_per_block = 1, # 모델의 크기 영향\n",
        "    block_out_channels = [64, 128, 256, 512], # 모델의 크기 영향\n",
        ")"
      ],
      "metadata": {
        "id": "8MVITo90HukW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `noisy_images`와 `timestep`을 입력으로 받아 예측된 노이즈를 출력합니다.\n"
      ],
      "metadata": {
        "id": "AUKZAH3KcHSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noisy_images = torch.randn((4, 1, 128, 128)) # 노이즈가 가미된 이미지 (테스트를 위해 랜덤 값으로 구성)\n",
        "timestep = torch.tensor([10]) # 1000 ~ 0\n",
        "\n",
        "# unet은 timestep에 맞게 노이즈가 첨가된 이미지에서 노이즈를 예측합니다.\n",
        "with torch.no_grad():\n",
        "    pred_noises = unet(noisy_images, timestep).sample\n",
        "print(pred_noises.shape) # 노이즈의 shape는 noisy_images 와 같아야 한다.\n",
        "show_img(pred_noises)"
      ],
      "metadata": {
        "id": "tfjS7JyjH9En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| Noise scheduler**\n",
        "\n",
        "두 번째 모듈은 **DDIM Noise Scheduler**입니다. **DDIM**은 Denoising Diffusion Implicit Models의 약자로, 이미지 생성 과정에서 점진적으로 노이즈를 제거하는 역할을 합니다. 이번 실습에서는 노이즈 추가 및 제거를 통해 이미지 재구성을 수행합니다. 모듈에 관한 자세한 내용은 [Hugging Face DDIM 페이지](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py)에서 확인할 수 있습니다.\n",
        "\n",
        "- `DDIMScheduler`를 사용하여 **Noise Scheduler**를 설정합니다. 이 모듈은 각 타임스텝에서 노이즈를 추가하거나 제거하는 역할을 합니다.\n",
        "- `num_train_timesteps`를 통해 전체 학습 타임스텝 수를 정의합니다.\n"
      ],
      "metadata": {
        "id": "7rnndEoYJOF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DModel, DDIMScheduler\n",
        "\n",
        "noise_scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
        "print('timesteps : ', len(noise_scheduler.timesteps), noise_scheduler.timesteps) # timesteps 확인"
      ],
      "metadata": {
        "id": "4xXPTp9KHg53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `set_timesteps()` 를 통해 사용할 타임스텝 수를 설정하고, 설정된 타임스텝을 출력합니다."
      ],
      "metadata": {
        "id": "eOv7Uz4Yc8Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_scheduler.set_timesteps(5) # noise scheduler timesteps 설정\n",
        "print('timesteps : ', len(noise_scheduler.timesteps), noise_scheduler.timesteps) # timesteps 확인"
      ],
      "metadata": {
        "id": "JqlQUkSPHi1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**노이즈 추가**\n",
        "\n",
        "- `noise_scheduler.timesteps`를 반복하면서 각 타임스텝마다 노이즈를 추가합니다. 이는 원래 이미지에 점진적으로 노이즈를 더해가며 모델을 훈련시키기 위한 과정입니다.\n",
        "- `torch.randn`을 사용하여 랜덤 노이즈를 생성합니다.\n",
        "- `noise_scheduler.add_noise` 메서드를 통해 노이즈 이미지로 변환합니다.\n",
        "- 노이즈 이미지를 시각화하여 확인합니다."
      ],
      "metadata": {
        "id": "8Kl656i_LvD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for timestep in noise_scheduler.timesteps:\n",
        "    print(timestep)\n",
        "    noises = torch.randn(img.shape) # 순수한 노이즈를 랜덤하게 생성\n",
        "    noisy_images = noise_scheduler.add_noise(img, noises, timestep) # 노이즈 스케줄러는 타임스텝에 맞게 적절한 노이즈를 이미지에 추가\n",
        "    show_img(noisy_images)"
      ],
      "metadata": {
        "id": "zzIPBf1nHkrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| Image Denoising Process**\n",
        "\n",
        "UNet과 DDIM Noise Scheduler를 사용하여 노이즈가 있는 이미지를 점진적으로 복원하는 과정 구현합니다. 가이드 코드는 [Hugging Face DDIM 파이프라인](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/ddim/pipeline_ddim.py)에서 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "xfBT4KiCJUps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 생성하는 과정.\n",
        "\n",
        "noise_scheduler.set_timesteps(4) # train 에서는 1000 step 사용, inference( 즉, 생성할 때는 )  보통 (20, 30, 50)\n",
        "\n",
        "images =  torch.randn((4, 1, 128, 128)) # 초기 noise 설정 (우리의 이미지는 노이즈로부터 생성될 예정)\n",
        "for timestep in noise_scheduler.timesteps: # timesteps 만큼 반복하기\n",
        "    print(timestep)\n",
        "    with torch.no_grad(): # 가중치 계산 비활성화\n",
        "        pred_noises = unet(images, timestep).sample # unet은 timestep 1000에서 거의 노이즈에 가까운 이미지로부터 노이즈만 예측\n",
        "        images  = noise_scheduler.step(pred_noises, timestep, images).prev_sample # 노이즈에 가까운 이미지에서 노이즈를 제거\n",
        "\n",
        "images = images.numpy()\n",
        "print(images.shape)\n",
        "show_img(images[:,0])\n",
        "\n",
        "# 이미지가 제대로 안나올 것 , 왜냐면 아직 unet을 학습하지 않았기 때문에."
      ],
      "metadata": {
        "id": "6wiX13c0ISV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(3) Model**"
      ],
      "metadata": {
        "id": "FtDFOFRqJbWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| Defining the Diffusion Model**"
      ],
      "metadata": {
        "id": "Pt84414hJvAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from diffusers import DDIMScheduler\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "\n",
        "from diffusers import UNet2DModel, DDIMScheduler\n",
        "import os\n",
        "\n",
        "class DiffusionModel(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 sample_size=128,\n",
        "                 in_channels=1,\n",
        "                 out_channels=1,\n",
        "                 layers_per_block=1,\n",
        "                 block_out_channels=[64, 128, 256, 512],\n",
        "                 num_train_timesteps=1000,\n",
        "                 device='cuda',\n",
        "                 seed =0):\n",
        "\n",
        "        super(DiffusionModel, self).__init__()\n",
        "        self.sample_size = sample_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.layers_per_block = layers_per_block\n",
        "        self.block_out_channels = block_out_channels\n",
        "        self.num_train_timesteps = num_train_timesteps\n",
        "        self.device_type = device\n",
        "\n",
        "        self.seed = seed\n",
        "\n",
        "        self.save_hyperparameters() # 하이퍼파라미터 값들이 자동 저장.\n",
        "\n",
        "        # 모듈 정의1. unet -> 학습할 모델\n",
        "        self.unet = UNet2DModel(\n",
        "            sample_size=sample_size,\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            layers_per_block=layers_per_block,\n",
        "            block_out_channels=block_out_channels,\n",
        "        )\n",
        "        # 모듈 정의2. noise scheduler\n",
        "        self.noise_scheduler = DDIMScheduler(num_train_timesteps=num_train_timesteps)\n",
        "\n",
        "    def forward(self, x, timesteps): # x: 노이즈~이미지\n",
        "        return self.unet(x, timesteps).sample\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.unet.parameters(), lr=self.learning_rate)\n",
        "        scheduler = get_cosine_schedule_with_warmup( # learning rate를 조정하는 스케줄러 != noise scheduler 다른개념!\n",
        "            optimizer,\n",
        "            num_warmup_steps=self.lr_warmup_steps,\n",
        "            num_training_steps=(self.num_training_steps * self.trainer.max_epochs),\n",
        "        )\n",
        "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        imgs = batch\n",
        "\n",
        "        # image에 들어갈 noise와 timestep 학습때마다 랜덤하게 정의 된다.\n",
        "        noise = torch.randn(imgs.shape).to(self.device)\n",
        "        timesteps = torch.randint(\n",
        "            0, self.noise_scheduler.config.num_train_timesteps,\n",
        "            (imgs.shape[0],),\n",
        "            device=self.device\n",
        "        ).long()\n",
        "\n",
        "        # 랜덤한 노이즈를 t 스텝에 맞게 적절히 이미지에 추가를 합니다.\n",
        "        noisy_images = self.noise_scheduler.add_noise(imgs, noise, timesteps)\n",
        "\n",
        "        # unet은 적절하게 노이즈가 가미된 이미지로 부터 순수한 랜덤 노이즈가 무엇인지 예측\n",
        "        noise_pred = self.unet(noisy_images, timesteps).sample\n",
        "\n",
        "        # 손실함수 : noise 를 unet이 예측하도록 함\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "        # Logging loss\n",
        "        lr = self.optimizers().param_groups[0]['lr']\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('learning_rate', lr, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def infer(self, n=4, seed=0, num_inference_steps=30):\n",
        "        generator = torch.manual_seed(seed)\n",
        "        self.noise_scheduler.set_timesteps(num_inference_steps)\n",
        "        shape = (n, self.in_channels, self.sample_size, self.sample_size)\n",
        "        images = torch.randn(shape, generator=generator).to(self.device)\n",
        "\n",
        "        for timestep in self.noise_scheduler.timesteps:\n",
        "            with torch.no_grad():\n",
        "                pred_noises = self.unet(images.to(self.device), timestep.to(self.device)).sample\n",
        "                images = self.noise_scheduler.step(pred_noises, timestep, images, generator=generator).prev_sample\n",
        "        return images\n",
        "\n",
        "    def fit(self, train_loader, save_dir, learning_rate=1e-4, lr_warmup_steps=10, num_epochs=10, patience=5, gradient_accumulation_steps=1):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lr_warmup_steps = lr_warmup_steps\n",
        "        self.num_training_steps = len(train_loader) // gradient_accumulation_steps\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        # Callbacks\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            dirpath=save_dir,\n",
        "            filename='unet_weights',\n",
        "            save_top_k=1,\n",
        "            verbose=True,\n",
        "            monitor='train_loss',\n",
        "            mode='min'\n",
        "        )\n",
        "\n",
        "        lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "        csv_logger = CSVLogger(save_dir, name=\"csv_logs\")\n",
        "\n",
        "        # Trainer\n",
        "        trainer = pl.Trainer(\n",
        "            accelerator='cuda',\n",
        "            max_epochs=num_epochs,\n",
        "            default_root_dir=save_dir,\n",
        "            callbacks=[checkpoint_callback, lr_monitor],\n",
        "            logger=[csv_logger],\n",
        "            log_every_n_steps=len(train_loader) // gradient_accumulation_steps,\n",
        "            gradient_clip_val=1.0,\n",
        "            accumulate_grad_batches=gradient_accumulation_steps,\n",
        "            precision=16,\n",
        "        )\n",
        "\n",
        "        trainer.fit(self, train_loader)\n",
        "\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        outputs = self.infer(seed=self.seed)\n",
        "        epoch = self.trainer.current_epoch\n",
        "        show_img(outputs, colorbar=True, save_path=f'{self.save_dir}/sample_epoch_{epoch:05d}.png')"
      ],
      "metadata": {
        "id": "z3vaxz-RIh81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| Training**"
      ],
      "metadata": {
        "id": "tUHWoLeYf4Kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 모델 객체 생성"
      ],
      "metadata": {
        "id": "ln3cG3Q8gJDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiffusionModel()"
      ],
      "metadata": {
        "id": "v3gfjB_mIk7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 모델 학습"
      ],
      "metadata": {
        "id": "j8dFcT04gMTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "paths= glob.glob('./PTN/*jpg')\n",
        "pp = PP(paths, resize=128, shuffle=True, return_condition=False)\n",
        "loader = pp.get_loader()\n",
        "\n",
        "model.fit(loader, './test_log', num_epochs=30)"
      ],
      "metadata": {
        "id": "FWxaSrm6ImIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| Generation**"
      ],
      "metadata": {
        "id": "sjvno95Af_y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 저장된 weight 불러오기"
      ],
      "metadata": {
        "id": "S3gQoTQxgPSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model =  DiffusionModel.load_from_checkpoint('./test_log/unet_weights.ckpt')\n",
        "# save hyperparameter 를 정의했기 때문에 weight 불러올 때 별도의 파라미터를 정이해주지 않아도 된다."
      ],
      "metadata": {
        "id": "RKwxtjwJL-dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 이미지 생성"
      ],
      "metadata": {
        "id": "MwepfQkogQ-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seed in range(3):\n",
        "  outputs = model.infer(seed=seed, num_inference_steps=20)\n",
        "  print(outputs.shape)\n",
        "  show_img(outputs, colorbar=True)"
      ],
      "metadata": {
        "id": "Q7O85ezKX6En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (4) Model : Stable Diffusion\n",
        "\n",
        "![](https://github.com/narnia-ai-eilie/box/blob/main/images/240214_sd_beyond.png?raw=true)"
      ],
      "metadata": {
        "id": "rNkf53quNepV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XZwOi7o9NfRX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
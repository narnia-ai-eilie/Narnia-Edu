{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPIf5Wb0BwagtF+FiF/fsaT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EilieYoun/Narnia-Edu/blob/main/Lecture/240822_kaist/01_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2024 KAIST 생성AI 실습 :  Diffusion\n",
        "\n",
        "* 날짜:\n",
        "* 이름:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1PpOHlxlnDvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(0) Environment Setup**\n",
        "---"
      ],
      "metadata": {
        "id": "XxQZXje__Db7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade diffusers[torch]"
      ],
      "metadata": {
        "id": "lQw5SIU3JnLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip carhood_npy.zip -d carhood"
      ],
      "metadata": {
        "id": "GRS7fwBBPOJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "\n",
        "\n",
        "def draw_img_from_cond(skin, geo, cmap='gray'):\n",
        "    plt.figure(figsize=(3,3))\n",
        "    path = f'carhood/skin_{skin}_geometry_{geo}.npy'\n",
        "    img = np.load(path)\n",
        "    plt.imshow(img, cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "    return img\n",
        "\n",
        "\n",
        "def draw_dist(inputs, samples=100):\n",
        "    if len(inputs) > samples:\n",
        "        inputs = random.sample(list(inputs), samples)\n",
        "\n",
        "    pixel_values = []\n",
        "    for item in inputs:\n",
        "        if isinstance(item, Image.Image):\n",
        "            pixel_values.extend(list(item.getdata()))\n",
        "        elif isinstance(item, np.ndarray):\n",
        "            pixel_values.extend(item.flatten())\n",
        "        elif torch.is_tensor(item):\n",
        "            if len(item.shape) == 4 and item.shape[1] == 1:  # 이미지가 (batch_size, 1, height, width) 꼴일 때\n",
        "                item = item.squeeze(1)\n",
        "            elif len(item.shape) == 3:  # 이미지가 (batch_size, height, width) 꼴일 때\n",
        "                item = item.unsqueeze(1)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported image shape. Supported shapes: (batch_size, height, width) or (batch_size, 1, height, width).\")\n",
        "            item = item.detach().cpu().numpy()\n",
        "            pixel_values.extend(item.flatten())\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported image type. Supported types: PIL Image, numpy array, torch tensor.\")\n",
        "\n",
        "    plt.figure(figsize=(5,3))\n",
        "    plt.hist(pixel_values, bins=20, color='blue', alpha=0.7)\n",
        "    plt.xlabel('Pixel Value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Image Pixel Value Distribution')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def show_img(images_list,\n",
        "             r=1,\n",
        "             cmap='gray',\n",
        "             img_size=(5, 5),\n",
        "             axis=\"off\",\n",
        "             colorbar=False,\n",
        "             colorbar_range=None,\n",
        "             save_path=None):\n",
        "    if r < 1:\n",
        "        r = 1\n",
        "\n",
        "    total_images = len(images_list)\n",
        "    if total_images == 0:\n",
        "        print(\"No images to display.\")\n",
        "        return\n",
        "\n",
        "    cols = (total_images + r - 1) // r\n",
        "    fig, axs = plt.subplots(r, cols, figsize=(cols * img_size[0], r * img_size[1]))\n",
        "\n",
        "    if r == 1:\n",
        "        axs = axs.reshape(1, -1)\n",
        "\n",
        "    for idx, item in enumerate(images_list):\n",
        "        ax = axs[0, idx] if r == 1 else axs[idx // cols, idx % cols]\n",
        "        im = None\n",
        "        if isinstance(item, Image.Image):\n",
        "            if item.mode in ['L', '1']:  # Grayscale images\n",
        "                im = ax.imshow(item, cmap=cmap)\n",
        "            else:  # Color images\n",
        "                im = ax.imshow(item)\n",
        "        elif isinstance(item, np.ndarray):\n",
        "            if item.ndim == 2:  # 2D array, grayscale image\n",
        "                im = ax.imshow(item, cmap=cmap, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 3:  # 3D array, color image\n",
        "                im = ax.imshow(item, cmap=cmap if item.shape[-1] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 4 and item.shape[0] == 1:  # 4D array with batch dimension of 1\n",
        "                im = ax.imshow(item[0], cmap=cmap if item.shape[1] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported numpy array shape: {item.shape}.\")\n",
        "        elif torch.is_tensor(item):\n",
        "            item = item.detach().cpu().numpy()\n",
        "            if item.ndim == 2:  # 2D tensor, grayscale image\n",
        "                im = ax.imshow(item, cmap=cmap, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 3:  # 3D tensor, color image\n",
        "                im = ax.imshow(item.transpose(1, 2, 0), cmap=cmap if item.shape[0] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 4 and item.shape[0] == 1:  # 4D tensor with batch dimension of 1\n",
        "                im = ax.imshow(item[0].transpose(1, 2, 0), cmap=cmap if item.shape[1] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported torch tensor shape: {item.shape}.\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported image type. Supported types: PIL Image, numpy array, torch tensor.\")\n",
        "\n",
        "        if colorbar and im is not None:\n",
        "            divider = make_axes_locatable(ax)\n",
        "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "            fig.colorbar(im, cax=cax)\n",
        "\n",
        "        ax.axis(axis)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # 이미지 저장\n",
        "    if save_path is not None:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "2O9iCBqiRxPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(1) Dataset**"
      ],
      "metadata": {
        "id": "vvsJn35E_F_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| EDA**\n",
        "\n",
        "**데이터 소개**\n",
        "\n",
        "\n",
        "`CarHoods10k` 데이터셋은 10,000개 이상의 자동차 후드 프레임 3D 메쉬 형상으로 구성되었으며, 자동화된 CAD 워크플로우를 통해 생성되었습니다. 이 데이터셋은 현실성과 제조 가능성에 대해 전문가의 검증을 거쳤습니다. 기초 형상 및 FEA(유한 요소 해석) 성능 지표가 포함되어 있으며, 자동차 공학 및 기타 분야에서 최적화 및 머신러닝 방법을 평가하고 개발하는 데 유용한 데이터를 제공합니다​ (DataDryad)​.\n",
        "\n",
        "`CarHoods10k` 데이터셋은 `Creative Commons Zero (CC0)` 라이센스로 제공됩니다. 이 라이센스는 사용자가 저작권을 포기한 것으로, 누구나 자유롭게 데이터를 복제, 수정, 배포, 심지어 상업적 목적으로도 사용할 수 있습니다. 사용자는 출처를 명시할 필요가 없으며, 데이터 사용에 대한 법적 제한이 없습니다\n",
        "\n",
        "이번 시간에 다룰 데이터는 `CarHood10k` 메쉬 데이터셋을 256x256 해상도의 depthmap으로 가공한 것입니다. 전체 데이터 중 무작위로 선택한 1,000개의 `depthmap npy` 파일을 사용하여 실습을 진행할 예정입니다."
      ],
      "metadata": {
        "id": "J4BD_y6coTZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = draw_img_from_cond(41, 29)\n",
        "print(img.shape)"
      ],
      "metadata": {
        "id": "8m48zdiHR5vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_dist([img])"
      ],
      "metadata": {
        "id": "D901-kvySP_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| DataLoader**"
      ],
      "metadata": {
        "id": "N4wEiGoxJGw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import glob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class PP(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 paths=[],\n",
        "                 resize=None,\n",
        "                 batch_size: int = 4,\n",
        "                 shuffle: bool = True,\n",
        "                 dtype = torch.float16,\n",
        "                ):\n",
        "\n",
        "        # init\n",
        "        self.paths = paths\n",
        "\n",
        "        self.resize = resize\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.dtype = dtype\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        path = self.paths[idx]\n",
        "        x = np.load(path)\n",
        "        x = x * 4. - 1.\n",
        "        x = torch.tensor(x, dtype=self.dtype).unsqueeze(0)\n",
        "        if self.resize is not None:\n",
        "            x = transforms.Resize(self.resize)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_loader(self):\n",
        "        return DataLoader(self, batch_size=self.batch_size, shuffle=self.shuffle)\n",
        "\n",
        "    def get_batch(self, idx=0):\n",
        "        ds = self.get_loader()\n",
        "        for i, batch in enumerate(ds):\n",
        "            if i == idx:\n",
        "                break\n",
        "        return batch"
      ],
      "metadata": {
        "id": "kBXB9HPJSizy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "paths= glob.glob('carhood/*npy')\n",
        "pp = PP(paths, resize=128)\n",
        "images = pp.get_batch(0)\n",
        "print(images.shape)\n",
        "show_img(images, colorbar=True)"
      ],
      "metadata": {
        "id": "nbz9a30YHdLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(2) Modules**\n",
        "\n",
        "\n",
        "\n",
        "![](https://github.com/EilieYoun/box/blob/main/images/240214_ddpm.png?raw=true)\n",
        "\n",
        "\n",
        "**Diffusion**은 이미지를 점진적으로 정제하는 과정을 통해 노이즈를 제거하고, 원래 이미지를 복원하는 생성 모델입니다. 이번 시간에서는 **DDIM** (Denoising Diffusion Implicit Models)을 사용하여 노이즈를 추가하고 제거하는 과정을 수행합니다.\n",
        "\n",
        "이번 실습에서는 **Hugging Face**에서 제공하는 `diffusers` 라이브러리를 사용하여 **Diffusion** 모델을 구축합니다. 이 라이브러리는 다양한 **Diffusion** 모델을 쉽게 구현하고 실험할 수 있는 도구를 제공합니다. `diffusers` 라이브러리는 모듈화된 구조를 가지고 있어, 다양한 모델 구성 요소를 쉽게 설정하고 사용할 수 있습니다. 자세한 내용은 https://github.com/huggingface 에서 확인할 수 있습니다.\n"
      ],
      "metadata": {
        "id": "iWGe2_DQJJXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| UNet**\n",
        "\n",
        "첫 번째 모듈은 UNet입니다. UNet은 이미지 분할 및 재구성 작업에 널리 사용되는 네트워크 구조로, 이번 실습에서는 노이즈를 예측하는 데 사용됩니다. 모듈에 관한 자세한 내용은 [Hugging Face UNET 페이지](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet/unet_2d.py)에서 확인할 수 있습니다.\n",
        "\n",
        "- `UNet2DModel`을 사용하여 **UNet** 모델을 정의합니다.\n",
        "- 입력이미지와 채널 수를 적절하게 정의해 줍니다.\n",
        "- `layers_per_block`과 `block_out_channels`를 기존보다 작게 설정해 가벼운 모델을 만듭니다."
      ],
      "metadata": {
        "id": "OOYECSxfJR4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "unet = UNet2DModel(\n",
        "    sample_size = 128,\n",
        "    in_channels = 1,\n",
        "    out_channels = 1,\n",
        "    layers_per_block = 1,\n",
        "    block_out_channels = [64, 128, 256, 512],\n",
        ")"
      ],
      "metadata": {
        "id": "8MVITo90HukW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `noisy_images`와 `timestep`을 입력으로 받아 예측된 노이즈를 출력합니다.\n"
      ],
      "metadata": {
        "id": "AUKZAH3KcHSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noisy_images = torch.randn((4, 1, 128, 128))\n",
        "timestep = torch.tensor([10])\n",
        "with torch.no_grad():\n",
        "    pred_noises = unet(noisy_images, timestep).sample\n",
        "print(pred_noises.shape)\n",
        "show_img(pred_noises)"
      ],
      "metadata": {
        "id": "tfjS7JyjH9En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| Noise scheduler**\n",
        "\n",
        "두 번째 모듈은 **DDIM Noise Scheduler**입니다. **DDIM**은 Denoising Diffusion Implicit Models의 약자로, 이미지 생성 과정에서 점진적으로 노이즈를 제거하는 역할을 합니다. 이번 실습에서는 노이즈 추가 및 제거를 통해 이미지 재구성을 수행합니다. 모듈에 관한 자세한 내용은 [Hugging Face DDIM 페이지](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py)에서 확인할 수 있습니다.\n",
        "\n",
        "- `DDIMScheduler`를 사용하여 **Noise Scheduler**를 설정합니다. 이 모듈은 각 타임스텝에서 노이즈를 추가하거나 제거하는 역할을 합니다.\n",
        "- `num_train_timesteps`를 통해 전체 학습 타임스텝 수를 정의합니다.\n"
      ],
      "metadata": {
        "id": "7rnndEoYJOF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DModel, DDIMScheduler\n",
        "\n",
        "noise_scheduler = DDIMScheduler(num_train_timesteps=1000)"
      ],
      "metadata": {
        "id": "4xXPTp9KHg53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `set_timesteps()` 를 통해 사용할 타임스텝 수를 설정하고, 설정된 타임스텝을 출력합니다."
      ],
      "metadata": {
        "id": "eOv7Uz4Yc8Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_scheduler.set_timesteps(5) # noise scheduler timesteps 설정\n",
        "print('timesteps : ', len(noise_scheduler.timesteps), noise_scheduler.timesteps) # timesteps 확인"
      ],
      "metadata": {
        "id": "JqlQUkSPHi1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**노이즈 추가**\n",
        "\n",
        "- `noise_scheduler.timesteps`를 반복하면서 각 타임스텝마다 노이즈를 추가합니다. 이는 원래 이미지에 점진적으로 노이즈를 더해가며 모델을 훈련시키기 위한 과정입니다.\n",
        "- `torch.randn`을 사용하여 랜덤 노이즈를 생성합니다.\n",
        "- `noise_scheduler.add_noise` 메서드를 통해 노이즈 이미지로 변환합니다.\n",
        "- 노이즈 이미지를 시각화하여 확인합니다."
      ],
      "metadata": {
        "id": "8Kl656i_LvD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for timestep in noise_scheduler.timesteps:\n",
        "    print(timestep)\n",
        "\n",
        "    noises = torch.randn(images.shape)\n",
        "    noisy_images = noise_scheduler.add_noise(images, noises, timestep)\n",
        "    show_img(noisy_images)"
      ],
      "metadata": {
        "id": "zzIPBf1nHkrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| Image Denoising Process**\n",
        "\n",
        "UNet과 DDIM Noise Scheduler를 사용하여 노이즈가 있는 이미지를 점진적으로 복원하는 과정 구현합니다. 가이드 코드는 [Hugging Face DDIM 파이프라인](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/ddim/pipeline_ddim.py)에서 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "xfBT4KiCJUps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_scheduler.set_timesteps(4) # inference steps 수 설정, 보통은 (20, 30, 50)\n",
        "images =  torch.randn((4, 1, 128, 128)) # 초기 noise 설정\n",
        "for timestep in noise_scheduler.timesteps: # timesteps 만큼 반복하기\n",
        "    print(timestep)\n",
        "    with torch.no_grad(): # 가중치 계산 비활성화\n",
        "        pred_noises = unet(images, timestep).sample # unet 작동\n",
        "        images  = noise_scheduler.step(pred_noises, timestep, images).prev_sample # noise 제거 step\n",
        "\n",
        "images = images.numpy()\n",
        "print(images.shape)\n",
        "show_img(images[:,0])"
      ],
      "metadata": {
        "id": "6wiX13c0ISV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(3) Model**"
      ],
      "metadata": {
        "id": "FtDFOFRqJbWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| Defining the Diffusion Model**"
      ],
      "metadata": {
        "id": "Pt84414hJvAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random, os\n",
        "\n",
        "from diffusers import DDIMScheduler\n",
        "from accelerate import Accelerator\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "\n",
        "class DiffusionModel():\n",
        "    def __init__(self,\n",
        "                 sample_size = 128,\n",
        "                 in_channels = 1,\n",
        "                 out_channels = 1,\n",
        "                 layers_per_block = 1,\n",
        "                 block_out_channels = [64, 128, 256, 512],\n",
        "                 num_train_timesteps=1000,\n",
        "                 dtype = torch.float16,\n",
        "                 device = 'cuda'\n",
        "                ):\n",
        "\n",
        "        # init\n",
        "        self.sample_size = sample_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.layers_per_block = layers_per_block\n",
        "        self.block_out_channels = block_out_channels\n",
        "        self.num_train_timesteps = num_train_timesteps\n",
        "\n",
        "        self.dtype = dtype\n",
        "        self.device = device\n",
        "        if self.dtype == torch.float16:\n",
        "            self.mp = 'fp16'\n",
        "        else:\n",
        "            self.mp = 'no'\n",
        "\n",
        "        # base modules\n",
        "        self.unet = UNet2DModel(\n",
        "            sample_size = sample_size,\n",
        "            in_channels = in_channels,\n",
        "            out_channels = out_channels,\n",
        "            layers_per_block = layers_per_block,\n",
        "            block_out_channels = block_out_channels,\n",
        "        )\n",
        "\n",
        "        self.noise_scheduler = DDIMScheduler(num_train_timesteps=num_train_timesteps)\n",
        "\n",
        "        # Initialize model\n",
        "        accelerator = Accelerator(mixed_precision=self.mp)\n",
        "        self.unet = accelerator.prepare(self.unet)\n",
        "\n",
        "\n",
        "    def load_unet_weight(self, path):\n",
        "        self.unet.load_state_dict(torch.load(path, map_location='cuda'))\n",
        "\n",
        "    def infer(self,\n",
        "              n=4,\n",
        "              seed=0,\n",
        "              num_inference_steps=30,\n",
        "              ):\n",
        "\n",
        "        generator=torch.manual_seed(seed)\n",
        "\n",
        "        self.noise_scheduler.set_timesteps(num_inference_steps)\n",
        "        shape = (n, self.in_channels, self.sample_size, self.sample_size)\n",
        "        images = torch.randn(shape, generator=generator).to(self.device) # 초기 noise 설정\n",
        "\n",
        "        for timestep in self.noise_scheduler.timesteps: # timesteps 만큼 반복하기\n",
        "            with torch.no_grad(): # 가중치 계산 비활성화\n",
        "                pred_noises = self.unet(images.to(self.device), timestep.to(self.device)).sample # unet 작동\n",
        "                images  = self.noise_scheduler.step(pred_noises, timestep, images, generator=generator).prev_sample # noise scheduler 작동\n",
        "\n",
        "        return images\n",
        "\n",
        "\n",
        "    def train( self,\n",
        "               dataset,\n",
        "               output_dir,\n",
        "               num_epochs = 10,\n",
        "               gradient_accumulation_steps = 1,\n",
        "               learning_rate = 1e-4,\n",
        "               lr_warmup_steps = 100,\n",
        "               seed = 0,\n",
        "             ):\n",
        "\n",
        "        # fix random seed\n",
        "        torch.manual_seed(seed)\n",
        "        random.seed(seed)\n",
        "\n",
        "        # Initialize accelerator\n",
        "        accelerator = Accelerator(\n",
        "            mixed_precision=self.mp,\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "            project_dir=os.path.join(output_dir, \"logs\"),\n",
        "        )\n",
        "\n",
        "        # opt, lr scheduler\n",
        "        optimizer = torch.optim.AdamW(self.unet.parameters(), lr=learning_rate)\n",
        "        lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer=optimizer,\n",
        "            num_warmup_steps=lr_warmup_steps,\n",
        "            num_training_steps=(len(dataset) * num_epochs),\n",
        "        )\n",
        "\n",
        "        self.noise_scheduler.set_timesteps(self.num_train_timesteps)\n",
        "\n",
        "        # to accelerator\n",
        "        self.unet, optimizer, dataset, lr_scheduler = accelerator.prepare(\n",
        "            self.unet, optimizer, dataset, lr_scheduler\n",
        "        )\n",
        "\n",
        "        # make output_dir\n",
        "        if output_dir is not None:\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Train loop\n",
        "        global_step = 0\n",
        "        for epoch in range(num_epochs):\n",
        "            progress_bar = tqdm(total=len(dataset))\n",
        "            progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "            for step, imgs in enumerate(dataset):\n",
        "\n",
        "                noise = torch.randn(imgs.shape).to(self.device)\n",
        "                timesteps = torch.randint(\n",
        "                    0, self.noise_scheduler.config.num_train_timesteps,\n",
        "                    (imgs.shape[0],),\n",
        "                    device=self.device\n",
        "                ).long()\n",
        "                noisy_images = self.noise_scheduler.add_noise(imgs, noise, timesteps)\n",
        "\n",
        "                with accelerator.accumulate(self.unet):\n",
        "\n",
        "                    noise_pred = self.unet(\n",
        "                        noisy_images,\n",
        "                        timesteps,\n",
        "                    ).sample\n",
        "\n",
        "                    loss = F.mse_loss(noise_pred, noise)\n",
        "                    accelerator.backward(loss)\n",
        "                    accelerator.clip_grad_norm_(self.unet.parameters(), 1.0)\n",
        "\n",
        "                    optimizer.step()\n",
        "                    lr_scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                # End batch\n",
        "                progress_bar.update(1)\n",
        "                logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "                progress_bar.set_postfix(**logs)\n",
        "                accelerator.log(logs, step=global_step)\n",
        "                global_step += 1\n",
        "\n",
        "\n",
        "            # End epoch\n",
        "            images = self.infer(seed=seed)\n",
        "            show_img(\n",
        "                images,\n",
        "                save_path=f\"{output_dir}/sample_{epoch:05d}.png\",\n",
        "                cmap=None,\n",
        "                colorbar=True,\n",
        "                axis='on',\n",
        "            )\n",
        "\n",
        "            torch.save(\n",
        "                self.unet.state_dict(),\n",
        "                f'{output_dir}/unet_weights.pth'\n",
        "            )"
      ],
      "metadata": {
        "id": "z3vaxz-RIh81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| Training**"
      ],
      "metadata": {
        "id": "tUHWoLeYf4Kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 모델 객체 생성"
      ],
      "metadata": {
        "id": "ln3cG3Q8gJDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiffusionModel()"
      ],
      "metadata": {
        "id": "v3gfjB_mIk7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 모델 학습"
      ],
      "metadata": {
        "id": "j8dFcT04gMTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = pp.get_loader()\n",
        "model.train(loader, './test', num_epochs=30)"
      ],
      "metadata": {
        "id": "FWxaSrm6ImIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **| Generation**"
      ],
      "metadata": {
        "id": "sjvno95Af_y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 저장된 weight 불러오기"
      ],
      "metadata": {
        "id": "S3gQoTQxgPSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_unet_weight('./test/unet_weights.pth')"
      ],
      "metadata": {
        "id": "RKwxtjwJL-dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 이미지 생성"
      ],
      "metadata": {
        "id": "MwepfQkogQ-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seed in range(3):\n",
        "  outputs = model.infer(seed=seed)\n",
        "  print(outputs.shape)\n",
        "  show_img(outputs, colorbar=True)"
      ],
      "metadata": {
        "id": "Q7O85ezKX6En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vrrRQiMmGniO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}